import json
import re
import os
import requests
import datetime

BOT_TOKEN = os.environ.get("BOT_TOKEN")
CHANNEL_ID = "@IAUCourseExp"
CLEAN_CH_ID = CHANNEL_ID.replace('@', '')
DATA_FILE = "src/data.json"


def clean_text(text):
    if not text: return ""
    return text.replace('#', '').replace('_', ' ').strip()

def parse_experience(message_text, msg_id):
    course_match = re.search(r"(?:ğŸ“š|ğŸŸ¡)\s*(?:Ù†Ø§Ù…\s+)?Ø¯Ø±Ø³\s*[:ï¼š]?\s*(.*?)(?=\n|ğŸ§®|ğŸŸ¢|ğŸ§‘â€ğŸ«|ğŸ”µ|$)", message_text, re.DOTALL)
    prof_match = re.search(r"(?:ğŸ§‘â€ğŸ«|ğŸ”µ)\s*(?:Ù†Ø§Ù…\s+Ø§Ø³ØªØ§Ø¯\s+Ù…Ø±Ø¨ÙˆØ·Ù‡|Ø§Ø³ØªØ§Ø¯)\s*[:ï¼š]?\s*(.*?)(?=\n|â“|ğŸ’¬|ğŸ”´|$)", message_text, re.DOTALL)
    
    student_score = "?"
    student_score_match = re.search(r"(?:ğŸ§®|ğŸŸ¢)\s*(?:Ù†Ù…Ø±Ù‡|Ù†Ù…Ø±ØªÙˆÙ†)\s*[:ï¼š]?\s*(.*?)(?=\n|ğŸ§‘â€ğŸ«|ğŸ”µ|â“|$)", message_text)
    if student_score_match:
        scores = re.findall(r"(\d+(?:\.\d+)?)", student_score_match.group(1).strip())
        student_score = scores[0] if scores else "?"

    prof_score = "?"
    prof_field_area = re.search(r"â“\s*Ù†Ù…Ø±Ù‡ ÛŒ Ø´Ù…Ø§ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯.*?(?=\n\s*(?:ğŸ’¬|ğŸ”´|ğŸ†”|$))", message_text, re.DOTALL)
    if prof_field_area:
        text_to_search = re.sub(r"\(.*?\d+.*?\d+.*?\)", "", prof_field_area.group(0))
        scores = re.findall(r"(\d+(?:\.\d+)?)", text_to_search)
        prof_score = scores[0] if scores else "?"

    text_match = re.search(r"(?:ğŸ’¬|ğŸ”´)\s*(?:ØªØ¬Ø±Ø¨Ù‡\s+Ø´Ù…Ø§|Ø¯ÛŒØ¯Ú¯Ø§Ù‡\s+Ø´Ù…Ø§|Ù†Ø¸Ø±ØªÙˆÙ†|Ù†Ø¸Ø±).*?[:ï¼š]\s*(.*?)(?=\n*-{5,}|\n*Ù„Ø·ÙØ§\s+Ø§Ø²\s+Ø·Ø±ÛŒÙ‚|$)", message_text, re.DOTALL)

    if course_match and prof_match:
        return {
            "id": 0,
            "Link": f"https://t.me/{CLEAN_CH_ID}/{msg_id}",
            "course": ' '.join(clean_text(course_match.group(1)).split()),
            "Student_Score": student_score,
            "Professor_Score": prof_score,
            "professor": prof_match.group(1).strip(),
            "text": re.sub(r"\n*â—ï¸ØªÙˆØ¬Ù‡â—ï¸.*", "", text_match.group(1).strip(), flags=re.DOTALL) if text_match else "Ø¨Ø¯ÙˆÙ† Ù…ØªÙ†"
        }
    return None

def scrape_with_bot():
    last_update_id = 0
    
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            database = json.load(f)
    else:
        database = []

    existing_links = {item['Link'] for item in database}

    url = f"https://api.telegram.org/bot{BOT_TOKEN}/getUpdates"
    
    try:
        response = requests.get(url, timeout=10).json()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ (Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø±ÙˆØ´Ù† Ù†ÛŒØ³Øª): {e}")
        return

    if not response.get("ok"):
        print(f"âŒ Ø®Ø·Ø§ Ø§Ø² Ø³Ù…Øª ØªÙ„Ú¯Ø±Ø§Ù…: {response.get('description')}")
        return

    if database:
        current_max_id = max(item['id'] for item in database)
    else:
        current_max_id = 0

    new_entries = []
    for update in response.get("result", []):
        last_update_id = update.get("update_id")
        message = update.get("channel_post")
        if not message: continue
        
        msg_id = message.get("message_id")
        msg_text = message.get("text", "")
        msg_link = f"https://t.me/{CLEAN_CH_ID}/{msg_id}"

        if msg_link in existing_links:
            continue

        if any(indicator in msg_text for indicator in ["ğŸ“šÙ†Ø§Ù… Ø¯Ø±Ø³", "ğŸŸ¡Ø¯Ø±Ø³"]):
            extracted = parse_experience(msg_text, msg_id)
            if extracted:
                current_max_id += 1
                extracted["id"] = current_max_id
                new_entries.append(extracted)
                
                new_item["id"] = current_max_id 

                new_entries.append(new_item)
                existing_links.add(msg_link)

    if last_update_id > 0:
        try:
            requests.get(f"https://api.telegram.org/bot{BOT_TOKEN}/getUpdates?offset={last_update_id + 1}", timeout=5)
            print(f"--- Ø¢Ù¾Ø¯ÛŒØªâ€ŒÙ‡Ø§ ØªØ§ Ø¢ÛŒâ€ŒØ¯ÛŒ {last_update_id} ØªØ§ÛŒÛŒØ¯ Ø´Ø¯Ù†Ø¯ ---")
        except:
            print("âš ï¸ Ø®Ø·Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø¯Ø± ØªØ§ÛŒÛŒØ¯ Ø¢Ù¾Ø¯ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…")

    if new_entries:
        database.extend(new_entries)
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(database, f, ensure_ascii=False, indent=4)
        
        now = datetime.datetime.utcnow() + datetime.timedelta(hours=3, minutes=30)
        update_info = {
            "last_update": now.strftime("%Y/%m/%d - %H:%M")
        }
        
        with open("src/last_update.json", "w", encoding="utf-8") as f:
            json.dump(update_info, f, ensure_ascii=False, indent=4)
        print(f"âœ… Ù…ÙˆÙÙ‚ÛŒØª: {len(new_entries)} ØªØ¬Ø±Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
    else:
        print("--- ØªØ¬Ø±Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ ---")

if __name__ == "__main__":
    scrape_with_bot()
